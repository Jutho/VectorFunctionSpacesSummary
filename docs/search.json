[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vector and Function Spaces – Summary",
    "section": "",
    "text": "For every chapter of the lecture notes “Vector- and function spaces”, this document lists the following items:\n\nA brief summary\nNot covered in class: sections which we have not or only briefly covered, and are not part of the exam (neither theory nor exercise). This corresponds to (subsections) that (should) have a (\\(\\star\\)) indicator in the lecture notes.\nImportant concepts: these mostly correspond to definitions, sometimes the definition is hidden inside a proposition or theorem. You do not need to know the literal definition, but you should be able to understand and use this term or concept correctly, both for theory and exercise.\nImportant theorems and propositions: list of proofs that need to be known (actively) or understood (passively) for the theory exam\nAdditional topics for applications / exercises: what you need to know for the exercises, in particular if it is additional material that you do not need to know for theory\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 26, 2023\n\n\nChapter 1 — Elementary algebraic structures\n\n\nJutho Haegeman\n\n\n\n\nOct 10, 2023\n\n\nChapter 2 — Linear maps and matrices\n\n\nJutho Haegeman\n\n\n\n\nOct 17, 2023\n\n\nChapter 3 — Linear operators and eigenvalues\n\n\nJutho Haegeman\n\n\n\n\nOct 17, 2023\n\n\nChapter 4 — Norms and distances\n\n\nJutho Haegeman\n\n\n\n\nNov 7, 2023\n\n\nChapter 5 — Inner products and orthogonality\n\n\nJutho Haegeman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "chapters/chapter5.html",
    "href": "chapters/chapter5.html",
    "title": "Chapter 5 — Inner products and orthogonality",
    "section": "",
    "text": "Summary\nThis important chapter introduces the concept of an inner product and the structures that follows from it, notabily, the concept of orthogonality and orthogonal projections. In particular, working with a basis in an infinite-dimensional vector space becomes more easy with an inner product (and associated norm) instead of “just a norm”, when using orthogonality. In a Hilbert space (=metric complete inner product space), a set of vectors that is complete (the linear span defines a dense subspace) can be turned into an orthonormal set (using Gram-Schmidt) which then defines a basis (expansion theorem). Linear maps and linear operators between Hilbert spaces can also have more structure. Bounded linear functionals (elements from the dual space) are one-to-one associated with vectors in the primal space via the inner product. Bounded linear maps have adjoints. Linear operators can satisfy relations with their adjoint, e.g. they can be equal (self-adjoint), the adjoint can be the inverse (unitary) or they can commute with the adjoint (normal), which then imposes particular constraints on the spectrum and the eigenvectors. On the practical side, the orthogonal projection allows to construct least square solutions to overdetermined systems.\n\n\nNot covered in class\nChapter 5 is one of the most important chapters of the course and has been covered completely.\n\n\nImportant concepts\n\nBilinear form, sesquilinear form, quadratic form, symmetric and Hermitian, degenerate, positive (semi)definite versus indefinite, matrix congruence (= basis transform for bilinear forms)\nInner product, standard/Euclidean inner product (in \\(\\mathbb{C}^n\\), in \\(\\ell^2\\), in \\(L^2\\)), metric/Gram matrix, inner product norm, continuity of the inner product, Hilbert space (=metric complete inner product space)\nOrthogonality, orthonormal set, orthogonal complement, orthogonal projection, orthogonal direct sum decomposition, orthogonal projector\nOrthonormal basis, Plancherel/Parseval identity, Gram-Schmidt orthonormalisation, QR decomposition\nRiesz representation theorem: (Anti)-isomorphism between dual space (= bounded linear functionals) and original Hilbert space\nBounded linear maps, operator norm expressed using inner product, bounded linear maps have closed kernels\nAdjoint of a linear map, self-adjoint operators, isometric and unitary maps, normal operator\nLeast squares solution, Moore-Penrose pseudoinverse\n\n\n\nLemmas, propositions, theorems\nChapter 5 contains some of the most important theorems and propositions (Cauchy-Schwarz inequality, orthogonal projection theorem, expansion theorem, …). The corresponding proofs are often short and constructive. Shorter proofs need to be known actively, longer proofs passively.\nActive proofs:\n\nTheorem 5.2: Cauchy-Schwarz inequality\nCorollary 5.3: inner product norm\nProposition 5.5: parallellogram law\nProposition 5.7 + corollary 5.8: linear independence of orthogonal vectors\nTheorem 5.9: Pythagoras\nProposition 5.10-5.12 as well as corollary 5.17-5.20: properties of orthogonal complement\nLemma 5.15: norm of an orthogonal projector\nLemma 5.21 and resulting from that Lemma 5.22: constructing orthogonal projection, Bessel’s inequality\nTheorem 5.23: expansion theorem\nCorollary 5.24 & 5.25: Plancherel and Parseval\nProposition 5.28: construction of the adjoint\nProposition 5.29 - 5.30: properties of the adjoint map\nProposition 5.31: characterisation of an linear isometry\nProposition 5.34: characterisation of a normal operator\nCorollary 5.35 and 5.36: structure of eigenvalues and eigenvectors of normal operators\nProposition 5.37, corollary 5.38 and 5.39: more properties of normal operators\n\nPassive proofs:\n\nTheorem 5.6: polarisation identity\nTheorem 5.13 + corollary 5.14: orthogonal projection + orthogonal direct sum\nTheorem 5.16: orthogonal projectors and direct sum\nTheorem 5.27: Riesz representation theorem\n\n\n\nFor applications / exercises\n\nComputing inner products, applying Gram-Schmidt (e.g. with custom inner products or between functions in a function space)\nKnowing and using the properties of self-adjoint and normal operators"
  },
  {
    "objectID": "chapters/chapter2.html",
    "href": "chapters/chapter2.html",
    "title": "Chapter 2 — Linear maps and matrices",
    "section": "",
    "text": "Summary\nThis chapter discusses general properties of vector space homomorphisms = linear maps, and in particular their representation as matrices in the case of finite-dimensional vector spaces. Most of this chapter should be repition, some parts are probably new (linear functionals and dual space, antilinear maps, determinants and inverses of block matrices). This chapter contains several important general concepts that you need to be able to actively use for both the theory and exercise exam.\n\n\nNot convered in class\nSection 2.5.4 (Double dual space) and Section 2.7.3 (Conjugate vector space) have not been covered in class.\n\n\nImportant concepts\n\nLinear map, property of linearity (\\(=\\) additivity + homogeneity), composition, identity\nKernel (\\(=\\) null space), image (\\(=\\) range), rank, nullity\nMatrix, matrix representation of a linear map\nLinear extensions\nMatrix multiplication, transpose, hermitian conjugate, (anti)symmetric and (anti)Hermitian matrix\nDeterminant (Leibniz formula) and trace\nAdjugate and inverse matrix, singular matrix (\\(=\\) zero determinant) and nonsingular matrix\nJacobi’s formula for the derivative of a determinant\nGeneral linear group (\\(=\\) invertible matrices), basis transform \\(=\\) similarity transform\nLinear functional, dual space (\\(=\\) linear functionals), basis transform of linear functional, dual linear map (\\(\\cong\\) matrix transpose)\nInterpretation of complex linear maps and vice versa; antilinear map\nSystem of linear equations, homogeneous and inhomogeneous, over- and underdetermined, upper and lower triangular matrix, LU decomposition\nBlock matrices, block-LDU decomposition and Schur complements, Sherman-Morrison-Woodbury identity\n\n\n\nLemmas, propositions and theorems\nSeveral of the propositions that are repititions of last year were not explicitly proven in the class. The most important propositions and theorems that we have given some attention and have proven in class are\n\nTheorem 2.11: Rank-nullity theorem \\(=\\) dimension theorem (together with proposition 2.10 which precedes it, and corollary 2.12)\nProposition 2.13: column rank = row rank (together with corollary 2.14)\nProposition 2.24: Jacobi’s formula\nProposition 2.30: Woodbury’s identity\n\n\n\nFor applications / exercises\n\nComputing determinant of jacobian for integration measures\nUsing Gaussian elemination, Schur complements, Sherman-Morrison-Woodbury formula"
  },
  {
    "objectID": "chapters/chapter3.html",
    "href": "chapters/chapter3.html",
    "title": "Chapter 3 — Linear operators and eigenvalues",
    "section": "",
    "text": "Summary\nThis chapter discusses general properties of vector space endomorphisms = linear operators = linear maps from a vector space to itself. Operators can be composed with themselves, giving rise to powers and polymials. This is important to then also introduce eigenvalues and eigenvectors, and finally, to generalise arbitrary scalar functions (\\(f:\\mathbb{C}\\to \\mathbb{C}\\)) to operators/square matrices. Parts of this will be repition (projectors, eigenvalues, …), other parts will be new (generalised eigenspaces, Jordan form, functions of operators).\n\n\nNot convered in class\n\nSection 3.2.5 (Jordan normal form) was only covered to give the end result, namely the specific structure of the Jordan normal form, without the “proof” or “recipe” of how it is obtained. You need to be able to use the Jordan form in applications, or in other theoretical constructions (like how to apply functions to it), but no proofs from section 3.2.5 need to be known.\nSection 3.2.6 (Sensitivity of eigenvalues and eigenspaces) was covered, but only gives a flavor of the difficulties related to the study of how the Jordan normal form changes under small perturbations. It does not contain any formal results, theorems or proofs.\nSection 3.3.3 (Derivatives of matrix functions) was not discussed at all in class, and thus does not need to be known.\nSection 3.4 was covered, but rather quickly, and mainly with how it should be used in applications and exercises (see below). There will be no theory questions from Section 3.4, but solving linear recurrence relations or differential equations is an important skill for the exercises.\n\n\n\nImportant concepts\n\nProjector and its relation to direct sum\nPolynomial of a matrix, annihilating polynomial, minimal annihilating polynomial\nEigenvalue, eigenvector, eigenspace, spectrum, geometric multiplicity, algebraic multiplicity, characteristic polynomial, spectral decomposition (= eigenvalue decomposition = diagonalisation), spectral projector, diagonalisable versus defective matrix\nInvariant subspace, generalised eigenspace, Jordan normal form\nCompanion matrix, eigenvectors of the companion matrix =&gt; diagonalised by Vandermonde matrix (not the generalisation for the case where eigenvalues coincide and the companion matrix is defective)\nStructure of eigenvalues and eigenvectors for real matrices\nLeft eigenvector and its relation to the dual of the linear operator\nFunction of an operator/matrix, matrix exponential, matrix logarithm and powers\n\n\n\nLemmas, propositions and theorems\nProofs to most lemmas and propositions are rather short and/or constructive and need to be known. Excluded from this is the construction of the Jordan normal form (Subsection 3.2.5).\nImportant constructive proofs (active knowledge):\n\nProposition 3.3: projector versus direct sum\nProposition 3.4\nProposition 3.7, 3.8 and corollary 3.9: linear independence of different eigenvectors\nProposition 3.10: eigenvalues of \\(\\hat{A}\\circ\\hat{B}\\) vs \\(\\hat{B}\\circ \\hat{A}\\)\nProposition 3.11: characteristic polynomial of a companion matrix\nTheorem 3.13: common spectral decomposition of commuting operators\n\nLonger proofs (passive knowledge):\n\nTheorem 3.12: Cayley Hamilton: passive knowledge of proof (understand but not reproduce)\nProposition 3.15: sequences of invariant subspaces\nTheorem 3.16: decomposition of \\(V\\) in invariant subspaces, interpretation of and relation between different indices \\(q_\\lambda\\), \\(r_\\lambda\\) and \\(s_\\lambda\\)\nProposition 3.19: Jordan decomposition of companion matrix\n\n\n\nFor applications / exercises\n\nUsing Cayley-Hamilton theorem\nComputing eigenvalues and eigenvectors\nComputing a matrix function via eigenvalue or Jordan decomposition\nSolving an (autonomous linear) initial value problem or recurrence relation (in particular, for the higher-order scalar case: Remark 3.62 and 3.69)"
  },
  {
    "objectID": "chapters/chapter1.html",
    "href": "chapters/chapter1.html",
    "title": "Chapter 1 — Elementary algebraic structures",
    "section": "",
    "text": "Summary\nThis chapter lists the basic mathematical structures and terminology that we will need and use, and doing so, also specifies the convention and notations that we will use for those. This should be almost completely repition; there will be no direct theory questions about this, but you should of course be able to use and understand this terminology\n\n\nNot convered in class\nSection 1.2.3 and 1.2.4 were only partially covered in class. We have only introduced group actions (and representations as special case), but not discussed the different properties it can have (faithful, free, transitive), nor the associated concepts of orbits and stabilizer subgroups. We have discussed the kernel of a group homomorphism and the special role of normal subgroups and quotient groups, but we have not at all discussed exact sequences.\n\n\nImportant concepts\n\nSet, subset, intersection, union\nMap, domain, codomain, argument, image, injective, surjective, bijective\nSet cardinality: finite, countably infinite, uncountable \\(=\\) uncountably infinite\nEquivalence relation (and partial order relation)\nBinary operation, associativity, commutativity, neutral element and inverses\nGroup, subgroup\nStructure preserving map \\(=\\) homomorphism, isomorphism, endomorphism, automorphism group\nkernel of a group homomorphism, normal subgroup\nPermutation, parity or signature of permutation\nRing, field\nVector space, linear combination, linear span, complete set, linear independence, basis, dimension, coordinates (coordinate isomorphism)\nLinear map, linear operator, linear transformation, general linear group\nSubspace, disjoint subspaces, direct sum, complement, codimension, quotient space,\nAlgebra, commutator\n\n\n\nLemmas, propositions and theorems\nNo proofs from Chapter 1\n\n\nFor applications / exercises\nUnderstanding the concepts and being able to recognise the relevant structures for exercises and examples"
  },
  {
    "objectID": "chapters/chapter4.html",
    "href": "chapters/chapter4.html",
    "title": "Chapter 4 — Norms and distances",
    "section": "",
    "text": "Summary\nThis chapter introduces the concept of a norm, and then discusses at length how this can be used to make sense of limits of sequences of vectors, mostly in infinite-dimensional vector spaces, where there are some surprises and not everything is very intuitive. Then, we discuss the implications for linear maps on such normed vector spaces.\n\n\nNot covered in class\nSection 4.4.1 was only discussed at high level. In section 4.4.2, I only mentioned the definition of the condition number. The contents of this section should be more or less equivalent to what is covered in Python 4 Scientists. If you are not following this course, it might be useful to read about the importance of the condition number.\nSection 4.4.3 was not discussed at all. If you want some theoretical background about Markov chains and the Google PageRank algorithm, feel free to read ;-).\n\n\nImportant concepts\n\nNorm, Hölder p-norm\nMetric (\\(=\\) distance), distance in a normed vector space, isometric map, limit of a sequence, continuity of a map, open subset, closed subset, closure, dense subset\nEquivalence of norms\nCauchy sequence, metric completeness (=every Cauchy sequence has limit), Banach space (=metric complete normed vector space), closed subspace, dense subspace, complete set, absolutely converging series, Schauder basis\nNorm for linear maps, Frobenius norm, bounded map \\(\\equiv\\) continuous map, subordinate norm, submultiplicative norm for operators, induced norm (\\(=\\) operator norm), spectral radius\nCondition number\n\nFinite-dimensional vector spaces: all norms are equivalent, always metric complete, finite-dimensional subspaces are always closed\nInfinite-dimensional vector spaces: proper infinite-dimensional subspaces of a Banach space can be dense: there is no intuitive or visual way to interpret this concept, there are vectors which are not in this subspace, but nonetheless they are arbitrary close to it (measured using the norm of the surrounding vector space).\nLinear maps: continuous if and only if bounded, bounded linear maps with operator norm are metric complete, induced norm is subordinate and submultiplicative, Gelfand formula for spectral radius\n\n\nLemmas, propositions, theorems\nNo proofs about compactness (Theorem 4.10, Lemma 4.11), metric completeness and Banach spaces (section 4.2 and also Proposition 4.19)\nActive proofs:\n\nHölder norms: inequalities of Young, Hölder and Minkowski (Lemma 4.1, lemma 4.2, proposition 4.3)\nContinuity of norm (proposition 4.5)\nCharacterisation of norm equivalence (Proposition 4.7)\nEquivalence between boundedness and continuity of linear maps (Proposition 4.17)\nBoundedness of linear maps with finite-dimensional domain (Proposition 4.18)\nSubmultiplicativity of operator norm (Proposition 4.20)\nConsistence of induced and Frobenius matrix norms (Proposition 4.21, 4.22)\nRelation between submultiplicative norms and spectral radius (Proposition 4.23, 4.24)\n\nPassive proofs:\n\nContinuity of vector addition and scalar addition (proposition 4.6)\nequivalence of all norms in finite dimensional spaces (proposition 4.8)\ndual norm (proposition 4.25)\n\nNo theory from section 4.4\n\n\nFor applications / exercises\n\nComputing norms\nProving inequality relations between standard \\(p\\) norms or computing induced norms for matrices.\n(But this chapter serves mostly theoretical purposes.)"
  }
]