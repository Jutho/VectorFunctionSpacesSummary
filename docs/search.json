[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vector and Function Spaces – Summary",
    "section": "",
    "text": "For every chapter of the lecture notes “Vector- and function spaces”, this document lists the following items:\n\nA brief summary\nNot covered in class: sections which we have not or only briefly covered, and are not part of the exam (neither theory nor exercise). This corresponds to (subsections) that (should) have a (\\(\\star\\)) indicator in the lecture notes.\nImportant concepts: these mostly correspond to definitions, sometimes the definition is hidden inside a proposition or theorem. You do not need to know the literal definition, but you should be able to understand and use this term or concept correctly, both for theory and exercise.\nImportant theorems and propositions: list of proofs that need to be known (actively) or understood (passively) for the theory exam\nAdditional topics for applications / exercises: what you need to know for the exercises, in particular if it is additional material that you do not need to know for theory\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 26, 2023\n\n\nChapter 1 — Elementary algebraic structures\n\n\nJutho Haegeman\n\n\n\n\nOct 10, 2023\n\n\nChapter 2 — Linear maps and matrices\n\n\nJutho Haegeman\n\n\n\n\nOct 17, 2023\n\n\nChapter 3 — Linear operators and eigenvalues\n\n\nJutho Haegeman\n\n\n\n\nOct 17, 2023\n\n\nChapter 4 — Norms and distances\n\n\nJutho Haegeman\n\n\n\n\nNov 7, 2023\n\n\nChapter 5 — Inner products and orthogonality\n\n\nJutho Haegeman\n\n\n\n\nNov 21, 2023\n\n\nChapter 6 — Unitary similarity and unitary equivalence\n\n\nJutho Haegeman\n\n\n\n\nNov 22, 2023\n\n\nChapter 7 — Function spaces\n\n\nJutho Haegeman\n\n\n\n\nDec 5, 2023\n\n\nChapter 8 — Linear differential operators\n\n\nJutho Haegeman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "chapters/chapter7.html",
    "href": "chapters/chapter7.html",
    "title": "Chapter 7 — Function spaces",
    "section": "",
    "text": "Summary\nChapter 7 discusses some of the consequences and issues related to working with functions spaces and infinite-dimensional Hilbert spaces more generally. Many of the results require proofs that are very technical and are beyond the scope of this course (even though they are included in the lecture notes for completeness). The goal is to get a certain amount of intuition about what it means to work with functions in \\(L^2([a,b])\\) or \\(L^2(\\mathbb{R})\\) (the important Hilbert space for quantum mechanics). This chapter deals with both the question of constructing an orthogonal basis (sections 7.3 and 7.4). A general function can then be expanded with respect to this basis, and thus gives rise to a sequence of expansion coefficients, which themselves live in the Hilbert space \\(\\ell^2(\\mathbb{N})\\) or \\(\\ell^2(\\mathbb{Z})\\). The final two sections discuss the different classes of operators that are relevant for such Hilbert spaces, and their properties, as well as adressing of several of the complications that arise, both with respect to their definition, the definition of the adjoint, and their spectral properties. Most of these issues are discussed in the context of differential operators and the interplay with boundary conditions, which is where they are most relevant in physics applications. The emphasis is on gaining intuition through these examples, rather than on the technical aspects of the general statements.\n\n\nNot covered in class\nWhile all sections where touched upon in class, several sections in the lecture notes contain technical results and their proofs which we did not discuss and are beyond the scope of this course (in particular everything in 7.1, subsections 7.4.2 and 7.4.3, section 7.6.1). Subsection 7.3.6 (Gaussian quadrature) was only covered up to (and including) Theoreom 7.13.\n\n\nImportant concepts\n\nFunction spaces can be given a proper norm and, for \\(L^2\\), an inner product. The non-trivial step involves `identifying’ functions that are equal almost everywhere, as one and the same (technically, working with equivalence classes of functions that are equal almost everywhere).\nThe function space \\(L^2\\) has interesting dense subspaces such as smooth or continuous functions, which are the ones we typically deal with\nPolynomials are dense (in Hilbert spaces where they are square integrable, either because of a finite interval or because of a proper weight function, or both) and can be turned into an orthogonal basis with a number of interesting properties (recurrence relation and structure of roots=zeros)\nTrigonometric polynomials are dense = Fourier modes are complete and thus the expansion theorem (Chapter 5 applies); unitary transformation between square integrable functions on an interval, and square summable sequence of Fourier coefficients (Parseval); Fourier coefficients have a number of interesting properties (translation, modulation, scaling, convolutions, …); Fourier series converge faster for smooth functions and have slow convergence for functions with discontinuities (Gibbs phenomenon); relation with discrete Fourier transform\nUnbounded operators are only defined on a subspace of the full Hilbert space = domain; the interesting class of operators are those for which that domain is still dense (=&gt; no vector is orthogonal to the domain).\nAlso the adjoint of an unbounded operator needs a domain, namely all \\(w\\) for which we can make \\(\\langle w, \\hat{A} v \\rangle = \\langle \\hat{A}^\\dagger w, v\\rangle\\) work for all \\(v\\) in the domain of \\(\\hat{A}\\). For differential operators, this relates to choosing boundary conditions for \\(v\\) and \\(w\\).\nUnderstanding the difference between being Hermitian/symmetric (\\(\\langle w, \\hat{A} v \\rangle = \\langle A w,v\\rangle\\) for all \\(v\\) and \\(w\\) in domain of \\(A\\)) and being self adjoint, again in the case of differential operators.\nThe spectrum of an operator \\(\\hat{A}\\) in an infinite-dimensionalHilbert space consists of three parts:\n\nThe point spectrum: actual eigenvalues \\(\\lambda\\) with normalizable eigenvectors \\(v\\): \\(\\hat{A} v = \\lambda v\\)\nThe continuous spectrum: values \\(\\lambda\\) for which we can find approximate eigenvectors, but no exact eigenvectors that we can properly normalize; we can find \\(v_\\epsilon\\) such that \\(\\lVert \\hat{A} v_\\epsilon - \\lambda v_\\epsilon \\rVert &lt; \\epsilon\\) for all \\(\\epsilon&gt;0\\), but the limit \\(\\epsilon \\to 0\\) of \\(v_\\epsilon\\) is not well defined\nThe residual spectrum: very unintuitive and related to the fact that, on infinite-dimensional Hilbert spaces \\(\\nu(\\hat{A})\\) (dimension of the kernel) and \\(\\nu(\\hat{A}^\\dagger)\\) do not need to be the same; the residual spectrum consists of values \\(\\lambda\\) for which no eigenvectors or approximate eigenvectors exist, but for which \\(\\overline{\\lambda}\\) is in the point spectrum of \\(\\hat{A}^\\dagger\\).\n\nFor a self adjoint operator, the residual spectrum is empty, the point spectrum is discrete and associated eigenvectors are orthonormal, and the point and continuous spectrum only contain real numbers.\n\n\n\nLemmas, propositions, theorems\nImportant active proofs:\n\nProposition 7.8, 7.9, 7.10, 7.11: properties of orthogonal polynomials (Cristoffel-Darboux formula does not need to be known by heart)\nProposition 7.12 (numerical integration)\nProposition 7.13 (Gaussian quadrature)\nProposition 7.17, 7.19, 7.24, 7.27: properties of the Fourier coefficients: proving the relation that the Fourier coefficients satisfy is an easy calculation, which you should be able to actively do. You do not need to know the technical conditions under which these manipulations are allowed, and which requirements they impose on the function \\(f\\) or the Fourier coefficients \\((\\widehat{f}_k)\\).\nProposition 7.31: operators with a commutator that yield the identity cannot be both bounded\n\n\n\nFor applications / exercises\n\nComputing inner products and applying Gram-Schmidt to a small set of functions.\nDeriving relations of specific families orthogonal polynomials, e.g. deriving orthonormalization relation or recurrence relation from generating function.\nComputing simple Fourier coefficients using the elementary properties\nDetermining whether a (differential) operator with given boundary conditions is symmetric and/or self-adjoint"
  },
  {
    "objectID": "chapters/chapter4.html",
    "href": "chapters/chapter4.html",
    "title": "Chapter 4 — Norms and distances",
    "section": "",
    "text": "Summary\nThis chapter introduces the concept of a norm, and then discusses at length how this can be used to make sense of limits of sequences of vectors, mostly in infinite-dimensional vector spaces, where there are some surprises and not everything is very intuitive. Then, we discuss the implications for linear maps on such normed vector spaces.\n\n\nNot covered in class\nSection 4.4.1 was only discussed at high level. In section 4.4.2, I only mentioned the definition of the condition number. The contents of this section should be more or less equivalent to what is covered in Python 4 Scientists. If you are not following this course, it might be useful to read about the importance of the condition number.\nSection 4.4.3 was not discussed at all. If you want some theoretical background about Markov chains and the Google PageRank algorithm, feel free to read ;-).\n\n\nImportant concepts\n\nNorm, Hölder p-norm\nMetric (\\(=\\) distance), distance in a normed vector space, isometric map, limit of a sequence, continuity of a map, open subset, closed subset, closure, dense subset\nEquivalence of norms\nCauchy sequence, metric completeness (=every Cauchy sequence has limit), Banach space (=metric complete normed vector space), closed subspace, dense subspace, complete set, absolutely converging series, Schauder basis\nNorm for linear maps, Frobenius norm, bounded map \\(\\equiv\\) continuous map, subordinate norm, submultiplicative norm for operators, induced norm (\\(=\\) operator norm), spectral radius\nCondition number\n\nFinite-dimensional vector spaces: all norms are equivalent, always metric complete, finite-dimensional subspaces are always closed\nInfinite-dimensional vector spaces: proper infinite-dimensional subspaces of a Banach space can be dense: there is no intuitive or visual way to interpret this concept, there are vectors which are not in this subspace, but nonetheless they are arbitrary close to it (measured using the norm of the surrounding vector space).\nLinear maps: continuous if and only if bounded, bounded linear maps with operator norm are metric complete, induced norm is subordinate and submultiplicative, Gelfand formula for spectral radius\n\n\nLemmas, propositions, theorems\nNo proofs about compactness (Theorem 4.10, Lemma 4.11), metric completeness and Banach spaces (section 4.2 and also Proposition 4.19)\nActive proofs:\n\nHölder norms: inequalities of Young, Hölder and Minkowski (Lemma 4.1, lemma 4.2, proposition 4.3)\nContinuity of norm (proposition 4.5)\nCharacterisation of norm equivalence (Proposition 4.7)\nEquivalence between boundedness and continuity of linear maps (Proposition 4.17)\nBoundedness of linear maps with finite-dimensional domain (Proposition 4.18)\nSubmultiplicativity of operator norm (Proposition 4.20)\nConsistence of induced and Frobenius matrix norms (Proposition 4.21, 4.22)\nRelation between submultiplicative norms and spectral radius (Proposition 4.23, 4.24)\n\nPassive proofs:\n\nContinuity of vector addition and scalar addition (proposition 4.6)\nequivalence of all norms in finite dimensional spaces (proposition 4.8)\ndual norm (proposition 4.25)\n\nNo theory from section 4.4\n\n\nFor applications / exercises\n\nComputing norms\nProving inequality relations between standard \\(p\\) norms or computing induced norms for matrices.\n(But this chapter serves mostly theoretical purposes.)"
  },
  {
    "objectID": "chapters/chapter1.html",
    "href": "chapters/chapter1.html",
    "title": "Chapter 1 — Elementary algebraic structures",
    "section": "",
    "text": "Summary\nThis chapter lists the basic mathematical structures and terminology that we will need and use, and doing so, also specifies the convention and notations that we will use for those. This should be almost completely repition; there will be no direct theory questions about this, but you should of course be able to use and understand this terminology\n\n\nNot convered in class\nSection 1.2.3 and 1.2.4 were only partially covered in class. We have only introduced group actions (and representations as special case), but not discussed the different properties it can have (faithful, free, transitive), nor the associated concepts of orbits and stabilizer subgroups. We have discussed the kernel of a group homomorphism and the special role of normal subgroups and quotient groups, but we have not at all discussed exact sequences.\n\n\nImportant concepts\n\nSet, subset, intersection, union\nMap, domain, codomain, argument, image, injective, surjective, bijective\nSet cardinality: finite, countably infinite, uncountable \\(=\\) uncountably infinite\nEquivalence relation (and partial order relation)\nBinary operation, associativity, commutativity, neutral element and inverses\nGroup, subgroup\nStructure preserving map \\(=\\) homomorphism, isomorphism, endomorphism, automorphism group\nkernel of a group homomorphism, normal subgroup\nPermutation, parity or signature of permutation\nRing, field\nVector space, linear combination, linear span, complete set, linear independence, basis, dimension, coordinates (coordinate isomorphism)\nLinear map, linear operator, linear transformation, general linear group\nSubspace, disjoint subspaces, direct sum, complement, codimension, quotient space,\nAlgebra, commutator\n\n\n\nLemmas, propositions and theorems\nNo proofs from Chapter 1\n\n\nFor applications / exercises\nUnderstanding the concepts and being able to recognise the relevant structures for exercises and examples"
  },
  {
    "objectID": "chapters/chapter3.html",
    "href": "chapters/chapter3.html",
    "title": "Chapter 3 — Linear operators and eigenvalues",
    "section": "",
    "text": "Summary\nThis chapter discusses general properties of vector space endomorphisms = linear operators = linear maps from a vector space to itself. Operators can be composed with themselves, giving rise to powers and polymials. This is important to then also introduce eigenvalues and eigenvectors, and finally, to generalise arbitrary scalar functions (\\(f:\\mathbb{C}\\to \\mathbb{C}\\)) to operators/square matrices. Parts of this will be repition (projectors, eigenvalues, …), other parts will be new (generalised eigenspaces, Jordan form, functions of operators).\n\n\nNot convered in class\n\nSection 3.2.5 (Jordan normal form) was only covered to give the end result, namely the specific structure of the Jordan normal form, without the “proof” or “recipe” of how it is obtained. You need to be able to use the Jordan form in applications, or in other theoretical constructions (like how to apply functions to it), but no proofs from section 3.2.5 need to be known.\nSection 3.2.6 (Sensitivity of eigenvalues and eigenspaces) was covered, but only gives a flavor of the difficulties related to the study of how the Jordan normal form changes under small perturbations. It does not contain any formal results, theorems or proofs.\nSection 3.3.3 (Derivatives of matrix functions) was not discussed at all in class, and thus does not need to be known.\nSection 3.4 was covered, but rather quickly, and mainly with how it should be used in applications and exercises (see below). There will be no theory questions from Section 3.4, but solving linear recurrence relations or differential equations is an important skill for the exercises.\n\n\n\nImportant concepts\n\nProjector and its relation to direct sum\nPolynomial of a matrix, annihilating polynomial, minimal annihilating polynomial\nEigenvalue, eigenvector, eigenspace, spectrum, geometric multiplicity, algebraic multiplicity, characteristic polynomial, spectral decomposition (= eigenvalue decomposition = diagonalisation), spectral projector, diagonalisable versus defective matrix\nInvariant subspace, generalised eigenspace, Jordan normal form\nCompanion matrix, eigenvectors of the companion matrix =&gt; diagonalised by Vandermonde matrix (not the generalisation for the case where eigenvalues coincide and the companion matrix is defective)\nStructure of eigenvalues and eigenvectors for real matrices\nLeft eigenvector and its relation to the dual of the linear operator\nFunction of an operator/matrix, matrix exponential, matrix logarithm and powers\n\n\n\nLemmas, propositions and theorems\nProofs to most lemmas and propositions are rather short and/or constructive and need to be known. Excluded from this is the construction of the Jordan normal form (Subsection 3.2.5).\nImportant constructive proofs (active knowledge):\n\nProposition 3.3: projector versus direct sum\nProposition 3.4\nProposition 3.7, 3.8 and corollary 3.9: linear independence of different eigenvectors\nProposition 3.10: eigenvalues of \\(\\hat{A}\\circ\\hat{B}\\) vs \\(\\hat{B}\\circ \\hat{A}\\)\nProposition 3.11: characteristic polynomial of a companion matrix\nTheorem 3.13: common spectral decomposition of commuting operators\n\nLonger proofs (passive knowledge):\n\nTheorem 3.12: Cayley Hamilton: passive knowledge of proof (understand but not reproduce)\nProposition 3.15: sequences of invariant subspaces\nTheorem 3.16: decomposition of \\(V\\) in invariant subspaces, interpretation of and relation between different indices \\(q_\\lambda\\), \\(r_\\lambda\\) and \\(s_\\lambda\\)\nProposition 3.19: Jordan decomposition of companion matrix\n\n\n\nFor applications / exercises\n\nUsing Cayley-Hamilton theorem\nComputing eigenvalues and eigenvectors\nComputing a matrix function via eigenvalue or Jordan decomposition\nSolving an (autonomous linear) initial value problem or recurrence relation (in particular, for the higher-order scalar case: Remark 3.62 and 3.69)"
  },
  {
    "objectID": "chapters/chapter2.html",
    "href": "chapters/chapter2.html",
    "title": "Chapter 2 — Linear maps and matrices",
    "section": "",
    "text": "Summary\nThis chapter discusses general properties of vector space homomorphisms = linear maps, and in particular their representation as matrices in the case of finite-dimensional vector spaces. Most of this chapter should be repition, some parts are probably new (linear functionals and dual space, antilinear maps, determinants and inverses of block matrices). This chapter contains several important general concepts that you need to be able to actively use for both the theory and exercise exam.\n\n\nNot convered in class\nSection 2.5.4 (Double dual space) and Section 2.7.3 (Conjugate vector space) have not been covered in class.\n\n\nImportant concepts\n\nLinear map, property of linearity (\\(=\\) additivity + homogeneity), composition, identity\nKernel (\\(=\\) null space), image (\\(=\\) range), rank, nullity\nMatrix, matrix representation of a linear map\nLinear extensions\nMatrix multiplication, transpose, hermitian conjugate, (anti)symmetric and (anti)Hermitian matrix\nDeterminant (Leibniz formula) and trace\nAdjugate and inverse matrix, singular matrix (\\(=\\) zero determinant) and nonsingular matrix\nJacobi’s formula for the derivative of a determinant\nGeneral linear group (\\(=\\) invertible matrices), basis transform \\(=\\) similarity transform\nLinear functional, dual space (\\(=\\) linear functionals), basis transform of linear functional, dual linear map (\\(\\cong\\) matrix transpose)\nInterpretation of complex linear maps and vice versa; antilinear map\nSystem of linear equations, homogeneous and inhomogeneous, over- and underdetermined, upper and lower triangular matrix, LU decomposition\nBlock matrices, block-LDU decomposition and Schur complements, Sherman-Morrison-Woodbury identity\n\n\n\nLemmas, propositions and theorems\nSeveral of the propositions that are repititions of last year were not explicitly proven in the class. The most important propositions and theorems that we have given some attention and have proven in class are\n\nTheorem 2.11: Rank-nullity theorem \\(=\\) dimension theorem (together with proposition 2.10 which precedes it, and corollary 2.12)\nProposition 2.13: column rank = row rank (together with corollary 2.14)\nProposition 2.24: Jacobi’s formula\nProposition 2.30: Woodbury’s identity\n\n\n\nFor applications / exercises\n\nComputing determinant of jacobian for integration measures\nUsing Gaussian elemination, Schur complements, Sherman-Morrison-Woodbury formula"
  },
  {
    "objectID": "chapters/chapter5.html",
    "href": "chapters/chapter5.html",
    "title": "Chapter 5 — Inner products and orthogonality",
    "section": "",
    "text": "Summary\nThis important chapter introduces the concept of an inner product and the structures that follows from it, notabily, the concept of orthogonality and orthogonal projections. In particular, working with a basis in an infinite-dimensional vector space becomes more easy with an inner product (and associated norm) instead of “just a norm”, when using orthogonality. In a Hilbert space (=metric complete inner product space), a set of vectors that is complete (the linear span defines a dense subspace) can be turned into an orthonormal set (using Gram-Schmidt) which then defines a basis (expansion theorem). Linear maps and linear operators between Hilbert spaces can also have more structure. Bounded linear functionals (elements from the dual space) are one-to-one associated with vectors in the primal space via the inner product. Bounded linear maps have adjoints. Linear operators can satisfy relations with their adjoint, e.g. they can be equal (self-adjoint), the adjoint can be the inverse (unitary) or they can commute with the adjoint (normal), which then imposes particular constraints on the spectrum and the eigenvectors. On the practical side, the orthogonal projection allows to construct least square solutions to overdetermined systems.\n\n\nNot covered in class\nChapter 5 is one of the most important chapters of the course and has been covered completely.\n\n\nImportant concepts\n\nBilinear form, sesquilinear form, quadratic form, symmetric and Hermitian, degenerate, positive (semi)definite versus indefinite, matrix congruence (= basis transform for bilinear forms)\nInner product, standard/Euclidean inner product (in \\(\\mathbb{C}^n\\), in \\(\\ell^2\\), in \\(L^2\\)), metric/Gram matrix, inner product norm, continuity of the inner product, Hilbert space (=metric complete inner product space)\nOrthogonality, orthonormal set, orthogonal complement, orthogonal projection, orthogonal direct sum decomposition, orthogonal projector\nOrthonormal basis, Plancherel/Parseval identity, Gram-Schmidt orthonormalisation, QR decomposition\nRiesz representation theorem: (Anti)-isomorphism between dual space (= bounded linear functionals) and original Hilbert space\nBounded linear maps, operator norm expressed using inner product, bounded linear maps have closed kernels\nAdjoint of a linear map, self-adjoint operators, isometric and unitary maps, normal operator\nLeast squares solution, Moore-Penrose pseudoinverse\n\n\n\nLemmas, propositions, theorems\nChapter 5 contains some of the most important theorems and propositions (Cauchy-Schwarz inequality, orthogonal projection theorem, expansion theorem, …). The corresponding proofs are often short and constructive. Shorter proofs need to be known actively, longer proofs passively.\nActive proofs:\n\nTheorem 5.2: Cauchy-Schwarz inequality\nCorollary 5.3: inner product norm\nProposition 5.5: parallellogram law\nProposition 5.7 + corollary 5.8: linear independence of orthogonal vectors\nTheorem 5.9: Pythagoras\nProposition 5.10-5.12 as well as corollary 5.17-5.20: properties of orthogonal complement\nLemma 5.15: norm of an orthogonal projector\nLemma 5.21 and resulting from that Lemma 5.22: constructing orthogonal projection, Bessel’s inequality\nTheorem 5.23: expansion theorem\nCorollary 5.24 & 5.25: Plancherel and Parseval\nProposition 5.28: construction of the adjoint\nProposition 5.29 - 5.30: properties of the adjoint map\nProposition 5.31: characterisation of an linear isometry\nProposition 5.34: characterisation of a normal operator\nCorollary 5.35 and 5.36: structure of eigenvalues and eigenvectors of normal operators\nProposition 5.37, corollary 5.38 and 5.39: more properties of normal operators\n\nPassive proofs:\n\nTheorem 5.6: polarisation identity\nTheorem 5.13 + corollary 5.14: orthogonal projection + orthogonal direct sum\nTheorem 5.16: orthogonal projectors and direct sum\nTheorem 5.27: Riesz representation theorem\n\n\n\nFor applications / exercises\n\nComputing inner products, applying Gram-Schmidt (e.g. with custom inner products or between functions in a function space)\nKnowing and using the properties of self-adjoint and normal operators"
  },
  {
    "objectID": "chapters/chapter6.html",
    "href": "chapters/chapter6.html",
    "title": "Chapter 6 — Unitary similarity and unitary equivalence",
    "section": "",
    "text": "Summary\nChapter 6 discusses some of the common matrix decompositions in finite-dimensional linear algebra. A significant fraction of the material deals with algorithmic details and remarks, and serves as background for or addition to e.g. Python for Scientists. It is not of high importance for either the theory or exercises of this course. However, some of the main theorems regarding Schur decomposition and singular value decomposition are important.\n\n\nNot covered in class\nSubsections 6.4.4 (practical considerations regarding Schur decomposition) and Section 6.7 (Krylov methods) were not at all covered in class. Subsection 6.6.6 (polar decomposition) was only briefly mentioned.\n\n\nImportant concepts\n\nUnitary and orthogonal group\nDiscrete Fourier transform as unitary transformation and circulant matrices\nSchur decomposition and its relation to eigenvalue decomposition for normal matrices\nCanonical form of a bilinear map, intertia or signature\nSingular value decomposition: full, thin and compact; relation to rank, operator norm, condition number; applicability in the context of least squares solution (pseudo-inverse) and low rank approximations\n\n\n\nLemmas, propositions, theorems\nImportant active proofs:\n\nProposition 6.2: Discrete Fourier transform diagonalises circulant matrices\nTheorem 6.3: Schur decomposition\nProposition 6.4: Normal matrices and Schur decomposition\nProposition 6.6 and theorem 6.7: Canonical form for congruence and uniqueness thereof (Sylvester’s law)\nProposition 6.8: Singular value decomposition (also remark 6.9 for its relation to eigenvalue / Schur decomposition)\nProposition 6.9: SVD and rank\nProposition 6.10: SVD and operator norm\nProposition 6.11: SVD and Frobenius norm\nProposition 6.12: SVD and condition number\nProposition 6.13: SVD and minimum norm least squares solution\nTheorem 6.14: SVD and low rank approximation in operator norm (not Theorem 6.15 in Frobenius norm)\n\n\n\nFor applications / exercises\nTechniques from Chapter 6 are mostly relevant for numerical work in linear algebra, and thus less so for the pen-and-paper exercises."
  },
  {
    "objectID": "chapters/chapter8.html",
    "href": "chapters/chapter8.html",
    "title": "Chapter 8 — Linear differential operators",
    "section": "",
    "text": "Summary\nThis chapter provides an in-depth study of differential operators, and their role in the study of linear differential equations with boundary conditions. We discuss how to construct the “formal adjoint” of a differential operator using partial integration, and how it relates to the boundary condition to actually construct the adjoint. We discuss how to decompose the solution of a differential equation in different parts, and how to study the existence and uniqueness of these different contributions. The adjoint plays a role via the Fredholm alternative theorem. Also, we discuss when a second order differential operator is self-adjoint (known as a Sturm-Liouville operator).\nTo better understand the role of boundary conditions, we take an extended detour via initial value problems, for which we can formally construct the solution (as a path ordered exponential), and we also discuss practical recipes (via a Taylor expansion or a generalisation thereof, known as Frobenius method). We find that we need \\(p\\) boundary conditions for a \\(p\\)th order differential equation to be well balanced. Finally then, we can construct the solution to the inhomogeneous differential equation (with homogeneous boundary conditions) using the Green’s function.\nWe then move on using differential operators in eigenvalue problems. The purpose thereof is for solving higher-dimensional partial differential equations, for which the spectral decomposion of a differential operator (if it exists) turns out to be very useful.\n\n\nNot covered in class\nWhile all subsections were mentioned in the lectures, several were only briefly mentioned and do not contain material that needs to be known for the theory exam. In particular, no theory from subsections 8.2.5 - 8.2.8 need to be known, but for the exercises, you need to know how to use a Taylor series and more generally the method of Frobenius to construct solutions of second order differential equations. Also subsections 8.4.3 and 8.4.4 as well as section 8.5 were only briefly mentioned.\n\n\nImportant concepts\n\nHomogeneous and inhomogeneous (linear) differential equation, homogeneous and inhomogeneous boundary conditions, formal adjoint of a linear differential operator, Sturm-Liouville operator, separated boundary conditions.\nA \\(p\\)th order differential equation is well balanced (likely to have a solution that exists and is unique for any right hand side) if it has exactly \\(p\\) boundary conditions; this is a necessary but not sufficient condition. Having \\(p\\) boundary conditions is also necessary (but not sufficient) to be a self-adjoint operator.\nUniqueness is associated with \\(\\mathrm{ker}(\\hat{L})\\), existence with \\(\\mathrm{ker}(\\hat{L}^\\dagger)\\) (using Fredholm’s alternative theorem).\nInitial value problem, fundamental matrix solution, propagator and time-ordered exponential, Wronskian, Floquet theorem\nBoundary value problem, Dirichlet and Neumann conditions for second order problems, Green’s function, Green’s operator as inverse of differential operator, adjoint of the Green’s operator\nSturm-Liouville eigenvalue problems: a regular Sturm-Liouville operator admits a spectral decompositoin where the eigenvectors provide a complete orthonormal basis for the Hilbert space, and can thus be used to compute e.g. the exponential or inverse (= Green’s function) (Remark 8.31 and 8.32)\n\n\n\nLemmas, propositions, theorems\nImportant active proofs:\n\nProposition 8.3 and Corollary 8.4 (it is sufficient if you can prove this for the case \\(p=2\\))\nProposition 8.5 and Corollary 8.6\nConstructing the bilinear concomitant of the Sturm-Liouville operator (eq 8.30)\nVerifying that it is self-adjoint with respect to separated or periodic boundary conditions.\nTheorem 8.12\nProposition 8.13\nProposition 8.14\nProposition 8.15\nProposition 8.16\nTheorem 8.17\n\nImportant passive proofs:\n\nProposition 8.9 and its generalisation to the inhomogeneous case, Proposition 8.18\n\nNo theorems beyond subsection 8.2.4; you need to understand examples and use the concepts (see above) in exercises.\n\n\nFor applications / exercises\n\nBe able to apply the Frobenius method, in particular for second order problems (Remarks 8.25 and 8.26). This requires that you can recognize/identify a regular singular point, and that you can derive the indicial equation. Also applying the simpler version (a Taylor series ansatz) when there is no singular point (coefficient of highest derivative is nowhere zero).\nUnderstand how to use a Green’s function, and how to construct it in the case of a second order problem with separated boundary conditions (p305 - 306, in particular eq 8.123).\nSolve (sufficiently simple) Sturm-Liouville problems and use the resulting spectral decomposition to compute the corresponding Green’s function or to solve time-dependent problems (computing \\(\\exp(t \\hat{L})\\))"
  }
]