[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vector and Function Spaces – Summary",
    "section": "",
    "text": "For every chapter of the lecture notes “Vector- and function spaces”, this document lists the following items:\n\nA brief summary\nNot covered in class: sections which we have not or only briefly covered, and are not part of the exam (neither theory nor exercise).\nImportant concepts: these mostly correspond to definitions, sometimes the definition is hidden inside a proposition or theorem. You do not need to know the literal definition, but you should be able to understand and use this term or concept correctly, both for theory and exercise.\nAdditional topics for applications / exercises: what you need to know for the exercises, in particular if it is additional material that you do not need to know for theory\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 24, 2024\n\n\nChapter 1 — Elementary algebraic structures\n\n\nJutho Haegeman\n\n\n\n\nOct 1, 2024\n\n\nChapter 2 — Linear maps and matrices\n\n\nJutho Haegeman\n\n\n\n\nOct 15, 2024\n\n\nChapter 3 — Linear operators and eigenvalues\n\n\nJutho Haegeman\n\n\n\n\nOct 17, 2023\n\n\nChapter 4 — Norms and distances\n\n\nJutho Haegeman\n\n\n\n\nNov 7, 2023\n\n\nChapter 5 — Inner products and orthogonality\n\n\nJutho Haegeman\n\n\n\n\nNov 21, 2023\n\n\nChapter 6 — Unitary similarity and unitary equivalence\n\n\nJutho Haegeman\n\n\n\n\nNov 22, 2023\n\n\nChapter 7 — Function spaces\n\n\nJutho Haegeman\n\n\n\n\nDec 5, 2023\n\n\nChapter 8 — Linear differential operators\n\n\nJutho Haegeman\n\n\n\n\nDec 12, 2023\n\n\nChapter 9 — Fourier calculus and distributions\n\n\nJutho Haegeman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "chapters/chapter9.html",
    "href": "chapters/chapter9.html",
    "title": "Chapter 9 — Fourier calculus and distributions",
    "section": "",
    "text": "Summary\nThis chapter introduces the Fourier transform, first in the classical sense (as a unitary operator on \\(L^2(\\mathbb{R})\\)). Then, the main concepts from the theory of distributions is introduced, which provides the mathematical framework for working with ‘generalised functions’ such as the Dirac delta ‘function’. Within the setting of distributions, we can significantly extend the concept of derivatives, limits, series and Fourier transforms beyond their classical meaning, and we provide several examples of this. Finally, we revisit the different types of Fourier transforms and introduce a fourth type that combines very naturally with the three types that we have already seen. Then, we find various relations between these different types, in which the use of distributions plays a prominent role.\n\n\nNot covered in class\nSubsections 9.2.11 and 9.3.5 and Section 9.4 have not been covered and do not need to be known.\n\n\nImportant concepts\n\nFourier transform and its elementary properties, convolution, Fourier transform as unitary operator on \\(L^2(\\mathbb{R})\\), Parseval and Plancherel relation for Fourier transform\nFourier transform of Gaussian distribution, characteristic function\nTest function, compact support, distribution, regular versus singular distribution, Dirac-delta distribution (and its derivatives), Heaviside function/distribution, Cauchy principal value, distributional derivative, distributional limit, distributional Fourier series and Fourier transform\nFourier transforms on different domains: discrete Fourier transform, Fourier series, discrete-time Fourier transform, (continuous-time) Fourier transform.\nSampling, Nyquist rate, reconstruction via sinc (Whittaker–Shannon interpolation formula).\n\n\n\nLemmas, propositions, theorems\n\nProperties of Fourier transform: Proposition 9.1 and 9.2, Theorem 9.3 (convolution), Proposition 9.4 (derivative) (not the technical requirements on \\(f\\) or \\(\\widehat{f}\\))\nComputing Fourier transform of Gaussian distribution, passive understanding of proving central limit theorem\nUsing the definition of translation, scaling, derivative, coordinate transform, limit of distributions on examples: Examples 9.9, 9.10, 9.11, 9.12, 9.13, Proposition 9.3, Examples 9.14, 9.15, 9.16, 9.17, 9.19\nTheorem 9.15 (given the structure of the complex logarithm and the distributional derivative of \\(\\log |x|\\))\nUsing Proposition 9.16 (Dirac sequence) on examples such as Example 9.20, 9.21, 9.22\nPassive understanding of subsection 9.2.5 (Cauchy Principal Value) and subsection 9.2.9 (Dirac Comb distribution)\nFourier transform of distributions on examples: Example 9.23, 9.24\nTheorem 9.18 (Poisson summation formula) given Dirac Comb distribution\nSampling: proving proposition 9.20, corollary 9.21 and proposition 9.22.\n\n\n\nFor applications / exercises\n\nComputing Fourier transforms using the few basic examples and a combination of their elementary properties.\nWorking with the Dirac distribution, Heaviside function, Dirac comb distribution (e.g. scaling or transforming the argument), …"
  },
  {
    "objectID": "chapters/chapter6.html",
    "href": "chapters/chapter6.html",
    "title": "Chapter 6 — Unitary similarity and unitary equivalence",
    "section": "",
    "text": "Summary\nChapter 6 discusses some of the common matrix decompositions in finite-dimensional linear algebra. A significant fraction of the material deals with algorithmic details and remarks, and serves as background for or addition to e.g. Python for Scientists. Some of its subsections focus on algorithmic or computational considerations, that are not important for either the theory or exercises of this course. However, some of the main theorems regarding Schur decomposition and singular value decomposition are important.\n\n\nNot covered in class\nSubsections 6.6.3 and 6.4.4 (practical considerations regarding Schur decomposition) and Section 6.7 (Krylov methods) were not at all covered in class. Subsection 6.6.6 (polar decomposition) was only briefly mentioned.\n\n\nImportant concepts\n\nUnitary and orthogonal group\nDiscrete Fourier transform as unitary transformation and circulant matrices\nSchur decomposition and its relation to eigenvalue decomposition for normal matrices\nCanonical form of a bilinear map, intertia or signature\nSingular value decomposition: full, thin and compact; relation to rank, operator norm, condition number; applicability in the context of least squares solution (pseudo-inverse) and low rank approximations\n\n\n\nFor applications / exercises\nTechniques from Chapter 6 are mostly relevant for numerical work in linear algebra, and thus less so for the pen-and-paper exercises."
  },
  {
    "objectID": "chapters/chapter5.html",
    "href": "chapters/chapter5.html",
    "title": "Chapter 5 — Inner products and orthogonality",
    "section": "",
    "text": "Summary\nThis important chapter introduces the concept of an inner product and the structures that follows from it, notabily, the concept of orthogonality and orthogonal projections. In particular, working with a basis in an infinite-dimensional vector space becomes more easy with an inner product (and associated norm) instead of “just a norm”, when using orthogonality. In a Hilbert space (=metric complete inner product space), a set of vectors that is complete (the linear span defines a dense subspace) can be turned into an orthonormal set (using Gram-Schmidt) which then defines a basis (expansion theorem). Linear maps and linear operators between Hilbert spaces can also have more structure. Bounded linear functionals (elements from the dual space) are one-to-one associated with vectors in the primal space via the inner product. Bounded linear maps have adjoints. Linear operators can satisfy relations with their adjoint, e.g. they can be equal (self-adjoint), the adjoint can be the inverse (unitary) or they can commute with the adjoint (normal), which then imposes particular constraints on the spectrum and the eigenvectors. On the practical side, the orthogonal projection allows to construct least square solutions to overdetermined systems.\n\n\nNot covered in class\nChapter 5 is one of the most important chapters of the course and has been covered completely.\n\n\nImportant concepts\n\nBilinear form, sesquilinear form, quadratic form, symmetric and Hermitian, degenerate, positive (semi)definite versus indefinite, matrix congruence (= basis transform for bilinear forms)\nInner product, standard/Euclidean inner product (in \\(\\mathbb{C}^n\\), in \\(\\ell^2\\), in \\(L^2\\)), metric/Gram matrix, inner product norm, continuity of the inner product, Hilbert space (=metric complete inner product space)\nOrthogonality, orthonormal set, orthogonal complement, orthogonal projection, orthogonal direct sum decomposition, orthogonal projector\nOrthonormal basis, Plancherel/Parseval identity, Gram-Schmidt orthonormalisation, QR decomposition\nRiesz representation theorem: (Anti)-isomorphism between dual space (= bounded linear functionals) and original Hilbert space\nBounded linear maps, operator norm expressed using inner product, bounded linear maps have closed kernels\nAdjoint of a linear map, self-adjoint operators, isometric and unitary maps, normal operator\nLeast squares solution, Moore-Penrose pseudoinverse\n\n\n\nFor applications / exercises\n\nComputing inner products, applying Gram-Schmidt (e.g. with custom inner products or between functions in a function space)\nKnowing and using the properties of self-adjoint and normal operators"
  },
  {
    "objectID": "chapters/chapter2.html",
    "href": "chapters/chapter2.html",
    "title": "Chapter 2 — Linear maps and matrices",
    "section": "",
    "text": "Summary\nThis chapter discusses general properties of vector space homomorphisms = linear maps, and in particular their representation as matrices in the case of finite-dimensional vector spaces. Most of this chapter should be repition, some parts are probably new (linear functionals and dual space, antilinear maps, determinants and inverses of block matrices). This chapter contains several important general concepts that you need to be able to actively use for both the theory and exercise exam.\n\n\nNot convered in class\nSection 2.5.4 (Double dual space) and Section 2.7.3 (Conjugate vector space) have not been covered in class.\n\n\nImportant concepts\n\nLinear map, property of linearity (\\(=\\) additivity + homogeneity), composition, identity\nKernel (\\(=\\) null space), image (\\(=\\) range), rank, nullity\nMatrix, matrix representation of a linear map\nLinear extensions\nMatrix multiplication, transpose, hermitian conjugate, (anti)symmetric and (anti)Hermitian matrix\nDeterminant (Leibniz formula) and trace\nAdjugate and inverse matrix, singular matrix (\\(=\\) zero determinant) and nonsingular matrix\nJacobi’s formula for the derivative of a determinant\nGeneral linear group (\\(=\\) invertible matrices), basis transform \\(=\\) similarity transform\nLinear functional, dual space (\\(=\\) linear functionals), basis transform of linear functional, dual linear map (\\(\\cong\\) matrix transpose)\nInterpretation of complex linear maps and vice versa; antilinear map\nSystem of linear equations, homogeneous and inhomogeneous, over- and underdetermined, upper and lower triangular matrix, LU decomposition\nBlock matrices, block-LDU decomposition and Schur complements, Sherman-Morrison-Woodbury identity\n\n\n\nFor applications / exercises\n\nComputing determinant of jacobian for integration measures\nUsing Gaussian elemination, Schur complements, Sherman-Morrison-Woodbury formula"
  },
  {
    "objectID": "chapters/chapter3.html",
    "href": "chapters/chapter3.html",
    "title": "Chapter 3 — Linear operators and eigenvalues",
    "section": "",
    "text": "Summary\nThis chapter discusses general properties of vector space endomorphisms = linear operators = linear maps from a vector space to itself. Operators can be composed with themselves, giving rise to powers and polymials. This is important to then also introduce eigenvalues and eigenvectors, and finally, to generalise arbitrary scalar functions (\\(f:\\mathbb{C}\\to \\mathbb{C}\\)) to operators/square matrices. Parts of this will be repition (projectors, eigenvalues, …), other parts will be new (generalised eigenspaces, Jordan form, functions of operators).\n\n\nNot convered in class\n\nSection 3.3.3 (Jordan normal form) was only covered to give the end result, namely the specific structure of the Jordan canonical form, without the “proof” or “recipe” of how it is obtained. You need to be able to use the Jordan form in applications, or in other theoretical constructions (like how to apply functions to it), but no proofs from section 3.3.3 need to be known.\nSection 3.3.4 (Sensitivity of eigenvalues and eigenspaces) was covered, but only gives a flavor of the difficulties related to the study of how the Jordan normal form changes under small perturbations. It does not contain any formal results, theorems or proofs.\nSection 3.4.3 (Derivatives of matrix functions) was not discussed at all in class, and thus does not need to be known.\nSection 3.5 was covered, but very quickly, with as only goal for it to be used in applications and exercises (see below). There will be no theory questions from Section 3.5, but solving linear recurrence relations or differential equations is an important skill for the exercises.\n\n\n\nImportant concepts\n\nProjector and its relation to direct sum\nPolynomial of a matrix, annihilating polynomial, minimal annihilating polynomial\nEigenvalue, eigenvector, eigenspace, spectrum, geometric multiplicity, algebraic multiplicity, characteristic polynomial, spectral decomposition (= eigenvalue decomposition = diagonalisation), spectral projector, diagonalisable versus defective matrix\nInvariant subspace, generalised eigenspace, Jordan normal form\nCompanion matrix, eigenvectors of the companion matrix =&gt; diagonalised by Vandermonde matrix (not the generalisation for the case where eigenvalues coincide and the companion matrix is defective)\nStructure of eigenvalues and eigenvectors for real matrices\nLeft eigenvector and its relation to the dual of the linear operator\nFunction of an operator/matrix, matrix exponential, matrix logarithm and powers\n\n\n\nFor applications / exercises\n\nUsing Cayley-Hamilton theorem\nComputing eigenvalues and eigenvectors\nComputing a matrix function via eigenvalue or Jordan decomposition\nSolving an (autonomous linear) initial value problem or recurrence relation (in particular, for the higher-order scalar case: Remark 3.59 and 3.66)"
  },
  {
    "objectID": "chapters/chapter1.html",
    "href": "chapters/chapter1.html",
    "title": "Chapter 1 — Elementary algebraic structures",
    "section": "",
    "text": "Summary\nThis chapter lists the basic mathematical structures and terminology that we will need and use, and doing so, also specifies the convention and notations that we will use for those. This should be almost completely repition; there will be no direct theory questions about this, but you should of course be able to use and understand this terminology\n\n\nNot convered in class\nSection 1.2.3 and 1.2.4 were only partially covered in class. We have only introduced group actions (and representations as special case), but not discussed the different properties it can have (faithful, free, transitive), nor the associated concepts of orbits and stabilizer subgroups. We have discussed the kernel of a group homomorphism and the special role of normal subgroups and quotient groups, but we have not at all discussed exact sequences.\n\n\nImportant concepts\n\nSet, subset, intersection, union\nMap, domain, codomain, argument, image, injective, surjective, bijective\nSet cardinality: finite, countably infinite, uncountable \\(=\\) uncountably infinite\nEquivalence relation (and partial order relation)\nBinary operation, associativity, commutativity, neutral element and inverses\nGroup, subgroup\nStructure preserving map \\(=\\) homomorphism, isomorphism, endomorphism, automorphism group\nkernel of a group homomorphism, normal subgroup\nPermutation, parity or signature of permutation\nRing, field\nVector space, linear combination, linear span, complete set, linear independence, basis, dimension, coordinates (coordinate isomorphism)\nLinear map, linear operator, linear transformation, general linear group\nSubspace, disjoint subspaces, direct sum, complement, codimension, quotient space,\nAlgebra, commutator\n\n\n\nFor applications / exercises\nUnderstanding the concepts and being able to recognise the relevant structures for exercises and examples"
  },
  {
    "objectID": "chapters/chapter4.html",
    "href": "chapters/chapter4.html",
    "title": "Chapter 4 — Norms and distances",
    "section": "",
    "text": "Summary\nThis chapter introduces the concept of a norm, and then discusses at length how this can be used to make sense of limits of sequences of vectors, mostly in infinite-dimensional vector spaces, where there are some surprises and not everything is very intuitive. Then, we discuss the implications for linear maps on such normed vector spaces.\n\n\nNot covered in class\nThe final page of section 4.1, on compactness was only briefly discussed (from Theorem 4.10 onwards). Section 4.2 (Banach spaces) was only discussed at high level, and no results need to be known from this section. Section 4.4 (Applications) was also largely skipped, up to some comments about Subsections 4.4.1 and 4.4.2. The only important concept is the condition number (Definition 4.24) and the way it apperas in Eq. 4.38. Section 4.4.3 was not discussed at all. If you want some theoretical background about Markov chains and the Google PageRank algorithm, feel free to read ;-).\n\n\nImportant concepts\n\nNorm, Hölder p-norm\nMetric (\\(=\\) distance), distance in a normed vector space, isometric map, limit of a sequence, continuity of a map, open subset, closed subset, closure, dense subset\nEquivalence of norms\nCauchy sequence, metric completeness (=every Cauchy sequence has limit), Banach space (=metric complete normed vector space), closed subspace, dense subspace, complete set, absolutely converging series, Schauder basis\nNorm for linear maps, Frobenius norm, bounded map \\(\\equiv\\) continuous map, subordinate norm, submultiplicative norm for operators, induced norm (\\(=\\) operator norm), spectral radius\nCondition number\n\nFinite-dimensional vector spaces: all norms are equivalent, always metric complete, finite-dimensional subspaces are always closed\nInfinite-dimensional vector spaces: proper infinite-dimensional subspaces of a Banach space can be dense: there is no intuitive or visual way to interpret this concept, there are vectors which are not in this subspace, but nonetheless they are arbitrary close to it (measured using the norm of the surrounding vector space).\nLinear maps: continuous if and only if bounded, bounded linear maps with operator norm are metric complete, induced norm is subordinate and submultiplicative, Gelfand formula for spectral radius\n\n\nFor applications / exercises\n\nComputing norms\nProving inequality relations between standard \\(p\\) norms or computing induced norms for matrices.\n(But this chapter serves mostly theoretical purposes.)"
  },
  {
    "objectID": "chapters/chapter7.html",
    "href": "chapters/chapter7.html",
    "title": "Chapter 7 — Function spaces",
    "section": "",
    "text": "Summary\nChapter 7 discusses some of the consequences and issues related to working with functions spaces and infinite-dimensional Hilbert spaces more generally. Many of the results require proofs that are very technical and are beyond the scope of this course (even though they are included in the lecture notes for completeness). The goal is to get a certain amount of intuition about what it means to work with functions in \\(L^2([a,b])\\) or \\(L^2(\\mathbb{R})\\) (the important Hilbert space for quantum mechanics). This chapter deals with both the question of constructing an orthogonal basis (sections 7.3 and 7.4). A general function can then be expanded with respect to this basis, and thus gives rise to a sequence of expansion coefficients, which themselves live in the Hilbert space \\(\\ell^2(\\mathbb{N})\\) or \\(\\ell^2(\\mathbb{Z})\\). The final two sections discuss the different classes of operators that are relevant for such Hilbert spaces, and their properties, as well as adressing of several of the complications that arise, both with respect to their definition, the definition of the adjoint, and their spectral properties. Most of these issues are discussed in the context of differential operators and the interplay with boundary conditions, which is where they are most relevant in physics applications. The emphasis is on gaining intuition through these examples, rather than on the technical aspects of the general statements.\n\n\nNot covered in class\nWhile all sections where touched upon in class, several sections in the lecture notes contain technical results and their proofs which we did not discuss and are beyond the scope of this course (in particular everything in 7.1, subsections 7.4.2 and 7.4.3, section 7.6.1). Subsection 7.3.6 (Gaussian quadrature) was only covered up to (and including) Theoreom 7.13.\n\n\nImportant concepts\n\nFunction spaces can be given a proper norm and, for \\(L^2\\), an inner product. The non-trivial step involves `identifying’ functions that are equal almost everywhere, as one and the same (technically, working with equivalence classes of functions that are equal almost everywhere).\nThe function space \\(L^2\\) has interesting dense subspaces such as smooth or continuous functions, which are the ones we typically deal with\nPolynomials are dense (in Hilbert spaces where they are square integrable, either because of a finite interval or because of a proper weight function, or both) and can be turned into an orthogonal basis with a number of interesting properties (recurrence relation and structure of roots=zeros)\nTrigonometric polynomials are dense = Fourier modes are complete and thus the expansion theorem (Chapter 5 applies); unitary transformation between square integrable functions on an interval, and square summable sequence of Fourier coefficients (Parseval); Fourier coefficients have a number of interesting properties (translation, modulation, scaling, convolutions, …); Fourier series converge faster for smooth functions and have slow convergence for functions with discontinuities (Gibbs phenomenon); relation with discrete Fourier transform\nUnbounded operators are only defined on a subspace of the full Hilbert space = domain; the interesting class of operators are those for which that domain is still dense (=&gt; no vector is orthogonal to the domain).\nAlso the adjoint of an unbounded operator needs a domain, namely all \\(w\\) for which we can make \\(\\langle w, \\hat{A} v \\rangle = \\langle \\hat{A}^\\dagger w, v\\rangle\\) work for all \\(v\\) in the domain of \\(\\hat{A}\\). For differential operators, this relates to choosing boundary conditions for \\(v\\) and \\(w\\).\nUnderstanding the difference between being Hermitian/symmetric (\\(\\langle w, \\hat{A} v \\rangle = \\langle A w,v\\rangle\\) for all \\(v\\) and \\(w\\) in domain of \\(A\\)) and being self adjoint, again in the case of differential operators.\nThe spectrum of an operator \\(\\hat{A}\\) in an infinite-dimensionalHilbert space consists of three parts:\n\nThe point spectrum: actual eigenvalues \\(\\lambda\\) with normalizable eigenvectors \\(v\\): \\(\\hat{A} v = \\lambda v\\)\nThe continuous spectrum: values \\(\\lambda\\) for which we can find approximate eigenvectors, but no exact eigenvectors that we can properly normalize; we can find \\(v_\\epsilon\\) such that \\(\\lVert \\hat{A} v_\\epsilon - \\lambda v_\\epsilon \\rVert &lt; \\epsilon\\) for all \\(\\epsilon&gt;0\\), but the limit \\(\\epsilon \\to 0\\) of \\(v_\\epsilon\\) is not well defined\nThe residual spectrum: very unintuitive and related to the fact that, on infinite-dimensional Hilbert spaces \\(\\nu(\\hat{A})\\) (dimension of the kernel) and \\(\\nu(\\hat{A}^\\dagger)\\) do not need to be the same; the residual spectrum consists of values \\(\\lambda\\) for which no eigenvectors or approximate eigenvectors exist, but for which \\(\\overline{\\lambda}\\) is in the point spectrum of \\(\\hat{A}^\\dagger\\).\n\nFor a self adjoint operator, the residual spectrum is empty, the point spectrum is discrete and associated eigenvectors are orthonormal, and the point and continuous spectrum only contain real numbers.\n\n\n\nLemmas, propositions, theorems\nImportant active proofs:\n\nProposition 7.8, 7.9, 7.10, 7.11: properties of orthogonal polynomials (Cristoffel-Darboux formula does not need to be known by heart)\nProposition 7.12 (numerical integration)\nProposition 7.13 (Gaussian quadrature)\nProposition 7.17, 7.19, 7.24, 7.27: properties of the Fourier coefficients: proving the relation that the Fourier coefficients satisfy is an easy calculation, which you should be able to actively do. You do not need to know the technical conditions under which these manipulations are allowed, and which requirements they impose on the function \\(f\\) or the Fourier coefficients \\((\\widehat{f}_k)\\).\nProposition 7.31: operators with a commutator that yield the identity cannot be both bounded\n\n\n\nFor applications / exercises\n\nComputing inner products and applying Gram-Schmidt to a small set of functions.\nDeriving relations of specific families orthogonal polynomials, e.g. deriving orthonormalization relation or recurrence relation from generating function.\nComputing simple Fourier coefficients using the elementary properties\nDetermining whether a (differential) operator with given boundary conditions is symmetric and/or self-adjoint"
  },
  {
    "objectID": "chapters/chapter8.html",
    "href": "chapters/chapter8.html",
    "title": "Chapter 8 — Linear differential operators",
    "section": "",
    "text": "Summary\nThis chapter provides an in-depth study of differential operators, and their role in the study of linear differential equations with boundary conditions. We discuss how to construct the “formal adjoint” of a differential operator using partial integration, and how it relates to the boundary condition to actually construct the adjoint. We discuss how to decompose the solution of a differential equation in different parts, and how to study the existence and uniqueness of these different contributions. The adjoint plays a role via the Fredholm alternative theorem. Also, we discuss when a second order differential operator is self-adjoint (known as a Sturm-Liouville operator).\nTo better understand the role of boundary conditions, we take an extended detour via initial value problems, for which we can formally construct the solution (as a path ordered exponential), and we also discuss practical recipes (via a Taylor expansion or a generalisation thereof, known as Frobenius method). We find that we need \\(p\\) boundary conditions for a \\(p\\)th order differential equation to be well balanced. Finally then, we can construct the solution to the inhomogeneous differential equation (with homogeneous boundary conditions) using the Green’s function.\nWe then move on using differential operators in eigenvalue problems. The purpose thereof is for solving higher-dimensional partial differential equations, for which the spectral decomposion of a differential operator (if it exists) turns out to be very useful.\n\n\nNot covered in class\nWhile all subsections were mentioned in the lectures, several were only briefly mentioned and do not contain material that needs to be known for the theory exam. In particular, no theory from subsections 8.2.5 - 8.2.8 need to be known, but for the exercises, you need to know how to use a Taylor series and more generally the method of Frobenius to construct solutions of second order differential equations. Also subsections 8.4.3 and 8.4.4 as well as section 8.5 were only briefly mentioned.\n\n\nImportant concepts\n\nHomogeneous and inhomogeneous (linear) differential equation, homogeneous and inhomogeneous boundary conditions, formal adjoint of a linear differential operator, Sturm-Liouville operator, separated boundary conditions.\nA \\(p\\)th order differential equation is well balanced (likely to have a solution that exists and is unique for any right hand side) if it has exactly \\(p\\) boundary conditions; this is a necessary but not sufficient condition. Having \\(p\\) boundary conditions is also necessary (but not sufficient) to be a self-adjoint operator.\nUniqueness is associated with \\(\\mathrm{ker}(\\hat{L})\\), existence with \\(\\mathrm{ker}(\\hat{L}^\\dagger)\\) (using Fredholm’s alternative theorem).\nInitial value problem, fundamental matrix solution, propagator and time-ordered exponential, Wronskian, Floquet theorem\nBoundary value problem, Dirichlet and Neumann conditions for second order problems, Green’s function, Green’s operator as inverse of differential operator, adjoint of the Green’s operator\nSturm-Liouville eigenvalue problems: a regular Sturm-Liouville operator admits a spectral decompositoin where the eigenvectors provide a complete orthonormal basis for the Hilbert space, and can thus be used to compute e.g. the exponential or inverse (= Green’s function) (Remark 8.31 and 8.32)\n\n\n\nLemmas, propositions, theorems\nImportant active proofs:\n\nProposition 8.3 and Corollary 8.4 (it is sufficient if you can prove this for the case \\(p=2\\))\nProposition 8.5 and Corollary 8.6\nConstructing the bilinear concomitant of the Sturm-Liouville operator (eq 8.30)\nVerifying that it is self-adjoint with respect to separated or periodic boundary conditions.\nTheorem 8.12\nProposition 8.13\nProposition 8.14\nProposition 8.15\nProposition 8.16\nTheorem 8.17\n\nImportant passive proofs:\n\nProposition 8.9 and its generalisation to the inhomogeneous case, Proposition 8.18\n\nNo theorems beyond subsection 8.2.4; you need to understand examples and use the concepts (see above) in exercises.\n\n\nFor applications / exercises\n\nBe able to apply the Frobenius method, in particular for second order problems (Remarks 8.25 and 8.26). This requires that you can recognize/identify a regular singular point, and that you can derive the indicial equation. Also applying the simpler version (a Taylor series ansatz) when there is no singular point (coefficient of highest derivative is nowhere zero).\nUnderstand how to use a Green’s function, and how to construct it in the case of a second order problem with separated boundary conditions (p305 - 306, in particular eq 8.123).\nSolve (sufficiently simple) Sturm-Liouville problems and use the resulting spectral decomposition to compute the corresponding Green’s function or to solve time-dependent problems (computing \\(\\exp(t \\hat{L})\\))"
  }
]